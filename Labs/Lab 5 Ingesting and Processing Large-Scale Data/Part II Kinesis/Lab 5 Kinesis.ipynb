{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 (Part II): Ingesting Streaming Data with Kinesis\n",
    "### MACS 30123: Large-Scale Computing for the Social Sciences (Spring 2020)\n",
    "\n",
    "In this second part of the lab, we'll explore how we can use Kinesis to ingest streaming text data, of the sort we might encounter on Twitter.\n",
    "\n",
    "To avoid requiring you to set up Twitter API access, we will create Twitter-like text and metadata using the `testdata` package to perform this demonstration. It should be easy enough to plug your streaming Twitter feed into this workflow if you desire to do so as an individual exercise (for instance, as a part of a final project!). Additionally, once you have this pipeline running, you can scale it up even further to include many more producers and consumers, if you would like, as discussed in lecture and the readings.\n",
    "\n",
    "Recall from the lecture and readings that in a Kinesis workflow, \"producers\" send data into a Kinesis stream and \"consumers\" draw data out of that stream to perform operations on it (i.e. real-time processing, archiving raw data, etc.). To make this a bit more concrete, we are going to implement a simplified version of this workflow in this lab, in which we spin up Producer and Consumer (t2.nano) EC2 Instances and create a Kinesis stream. Our Producer instance will run a producer script (which writes our Twitter-like text data into a Kinesis stream) and our Consumer instance will run a consumer script (which reads the Twitter-like data and calculates a simple demo statistic -- the average unique word count per tweet, as a real-time running average).\n",
    "\n",
    "You can visualize this data pipeline, like so:\n",
    "\n",
    "<img src=\"simple_kinesis_architecture.png\" width=\"800\" align=\"left\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin implementing this pipeline, let's import `boto3` and initialize the AWS services we'll be using in this lab (EC2 and Kinesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "session = boto3.Session()\n",
    "\n",
    "kinesis = session.client('kinesis')\n",
    "ec2 = session.resource('ec2')\n",
    "ec2_client = session.client('ec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create the Kinesis stream that our Producer EC2 instance will write streaming tweets to. Because we're only setting this up to handle traffic from one consumer and one producer, we'll just use one shard, but we could increase our throughput capacity by increasing the ShardCount if we wanted to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = kinesis.create_stream(StreamName = 'test_stream',\n",
    "                                 ShardCount = 1\n",
    "                                )\n",
    "\n",
    "# Is the stream active and ready to be written to/read from? Wait until it exists before moving on:\n",
    "waiter = kinesis.get_waiter('stream_exists')\n",
    "waiter.wait(StreamName='test_stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we're ready to set up our producer and consumer EC2 instances that will write to and read from this Kinesis stream. Let's spin up our two EC2 instances (specified by the `MaxCount` parameter) using one of the Amazon Linux AMIs.  Notice here that you will need to specify your `.pem` file for the `KeyName` parameter, as well as create a custom security group/group ID. Designating a security group is necessary because, by default, AWS does not allow inbound ssh traffic into EC2 instances (they create custom ssh-friendly security groups each time you run the GUI wizard in the console). Thus, if you don't set this parameter, you will not be able to ssh into the EC2 instances that you create here with `boto3`. You can follow along in the lab video for further instructions on how you can set up one of these security groups.\n",
    "\n",
    "Also, we need to specify an IAM Instance Profile so that our EC2 instances will have the permissions necessary to interact with other AWS services on our behalf. Here, I'm using one of the profiles we create in Part I of Lab 5 (a default AWS profile for launching EC2 instances within an EMR cluster), as this gives us all of the necessary permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ec2.create_instances(ImageId='ami-0915e09cc7ceee3ab',\n",
    "                                 MinCount=1,\n",
    "                                 MaxCount=2,\n",
    "                                 InstanceType='t2.micro',\n",
    "                                 KeyName='MACS_30123',\n",
    "                                 SecurityGroupIds=['sg-02248fb2c9eac8bef'],\n",
    "                                 SecurityGroups=['MACS_30123'],\n",
    "                                 IamInstanceProfile=\n",
    "                                     {'Name': 'EMR_EC2_DefaultRole'},\n",
    "                                )\n",
    "\n",
    "# Wait until EC2 instances are running before moving on\n",
    "waiter = ec2_client.get_waiter('instance_running')\n",
    "waiter.wait(InstanceIds=[instance.id for instance in instances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we wait for these instances to start running, let's set up the Python scripts that we want to run on each instance. First of all, we have to define a script for our Producer instance, which continuously produces Twitter-like data using the `testdata` package and puts that data into our Kinesis stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting producer.py\n"
     ]
    }
   ],
   "source": [
    "%%file producer.py\n",
    "\n",
    "import boto3\n",
    "import testdata\n",
    "import json\n",
    "\n",
    "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
    "\n",
    "# Continously write Twitter-like data into Kinesis stream\n",
    "while 1 == 1:\n",
    "    test_tweet = {'username': testdata.get_username(),\n",
    "                  'tweet':    testdata.get_ascii_words(280)\n",
    "                  }\n",
    "    kinesis.put_record(StreamName = \"test_stream\",\n",
    "                       Data = json.dumps(test_tweet),\n",
    "                       PartitionKey = \"partitionkey\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define a script for our Consumer instance that gets the latest tweet out of the stream, one at a time. After processing each tweet, we then print out the average unique word count per processed tweet as a running average, before jumping on to the next indexed tweet in our Kinesis stream shard to do the same thing for as long as our program is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer.py\n"
     ]
    }
   ],
   "source": [
    "%%file consumer.py\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "\n",
    "kinesis = boto3.client('kinesis', region_name='us-east-1')\n",
    "\n",
    "shard_it = kinesis.get_shard_iterator(StreamName = \"test_stream\",\n",
    "                                     ShardId = 'shardId-000000000000',\n",
    "                                     ShardIteratorType = 'LATEST'\n",
    "                                     )[\"ShardIterator\"]\n",
    "\n",
    "i = 0\n",
    "s = 0\n",
    "while 1==1:\n",
    "    out = kinesis.get_records(ShardIterator = shard_it,\n",
    "                              Limit = 1\n",
    "                             )\n",
    "    for o in out['Records']:\n",
    "        jdat = json.loads(o['Data'])\n",
    "        s = s + len(set(jdat['tweet'].split()))\n",
    "        i = i + 1\n",
    "\n",
    "    if i != 0:\n",
    "        print(\"Average Unique Word Count Per Tweet: \" + str(s/i))\n",
    "        print(\"Sample of Current Tweet: \" + jdat['tweet'][:20])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    shard_it = out['NextShardIterator']\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our final preparation step, we'll grab all of the public DNS names of our instances (web addresses that you normally copy from the GUI console to manually ssh into  and record the names of our code files, so that we can easily ssh/scp into the instances and pass them our Python scripts to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dns = [instance.public_dns_name \n",
    "                 for instance in ec2.instances.all() \n",
    "                 if instance.state['Name'] == 'running'\n",
    "               ]\n",
    "\n",
    "code = ['producer.py', 'consumer.py']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To copy our files over to our instances and programmatically run commands on them, we can use Python's `scp` and `paramiko` packages. You'll need to install these via `pip install paramiko scp` if you have not already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paramiko in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (2.7.1)\n",
      "Requirement already satisfied: scp in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (0.13.2)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (from paramiko) (3.1.7)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (from paramiko) (1.3.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (from paramiko) (2.8)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (from bcrypt>=3.1.3->paramiko) (1.14.0)\n",
      "Requirement already satisfied: cffi>=1.1 in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (from bcrypt>=3.1.3->paramiko) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /home/jclindaniel/anaconda3/lib/python3.7/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko) (2.19)\n"
     ]
    }
   ],
   "source": [
    "! pip install paramiko scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `scp` and `paramiko` installed, we can copy our producer and consumer Python scripts over to the EC2 instances (designating our first EC2 instance in `instance_dns` as the producer and second EC2 instance as the consumer instance).\n",
    "\n",
    "Note that, on each instance, we install `boto3` (so that we can access Kinesis through our scripts) and then copy our producer/consumer Python code over to our producer/consumer EC2 instance via `scp`. After we've done this, we install the `testdata` package on the producer instance (which it needs in order to create fake tweets) and instruct it to run our Python producer script. This will write tweets into our Kinesis stream until we stop the script and terminate the producer EC2 instance.\n",
    "\n",
    "We could also instruct our consumer to get tweets from the stream immediately after this command and this would automatically collect and process the tweets according to the consumer.py script. For the purposes of this demonstration, though, we'll manually ssh into that instance and run the code from the terminal so that we can see the real-time consumption a bit more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer Instance is Running producer.py\n",
      ".........................................\n",
      "Connect to Consumer Instance by running: ssh -i \"MACS_30123.pem\" ec2-user@ec2-52-207-178-178.compute-1.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "from scp import SCPClient\n",
    "ssh_producer, ssh_consumer = paramiko.SSHClient(), paramiko.SSHClient()\n",
    "\n",
    "# Initialization of SSH tunnels takes a bit of time; otherwise get connection error on first attempt\n",
    "time.sleep(5)\n",
    "\n",
    "# Install boto3 on each EC2 instance and Copy our producer/consumer code onto producer/consumer EC2 instances\n",
    "instance = 0\n",
    "stdin, stdout, stderr = [[None, None] for i in range(3)]\n",
    "for ssh in [ssh_producer, ssh_consumer]:\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(instance_dns[instance],\n",
    "                username = 'ec2-user',\n",
    "                key_filename='/home/jclindaniel/MACS_30123.pem')\n",
    "    \n",
    "    with SCPClient(ssh.get_transport()) as scp:\n",
    "        scp.put(code[instance])\n",
    "    \n",
    "    if instance == 0:\n",
    "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
    "            ssh.exec_command(\"sudo pip install boto3 testdata\")\n",
    "    else:\n",
    "        stdin[instance], stdout[instance], stderr[instance] = \\\n",
    "            ssh.exec_command(\"sudo pip install boto3\")\n",
    "\n",
    "    instance += 1\n",
    "\n",
    "# Block until Producer has installed boto3 and testdata, then start running Producer script:\n",
    "producer_exit_status = stdout[0].channel.recv_exit_status() \n",
    "if producer_exit_status == 0:\n",
    "    ssh_producer.exec_command(\"python %s\" % code[0])\n",
    "    print(\"Producer Instance is Running producer.py\\n.........................................\")\n",
    "else:\n",
    "    print(\"Error\", producer_exit_status)\n",
    "\n",
    "# Close ssh and show connection instructions for manual access to Consumer Instance\n",
    "ssh_consumer.close; ssh_producer.close()\n",
    "\n",
    "print(\"Connect to Consumer Instance by running: ssh -i \\\"MACS_30123.pem\\\" ec2-user@%s\" % instance_dns[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the command above (with the correct path to your actual `.pem` file), you should be inside your Consumer EC2 instance. If you run `python consumer.py`, you should also see a real-time count of the average number of unique words per tweet (along with a sample of the text in the most recent tweet), as in the screenshot:\n",
    "\n",
    "![](consumer_feed.png)\n",
    "\n",
    "Cool! Now we can scale this basic architecture up to perform any number of real-time data analyses, if we so desire. Also, if we execute our consumer code remotely via paramiko as well, the process will be entirely remote, so we don't need to keep any local resources running in order to keep streaming/processing real-time data.\n",
    "\n",
    "As a final note, when you are finished observing the real-time feed from your consumer instance, **be sure to terminate your EC2 instances and delete your Kinesis stream**. You don't want to be paying for these to run continuously! You can do so programmatically by running the following `boto3` code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC2 Instances Successfully Terminated\n",
      "Kinesis Stream Successfully Deleted\n"
     ]
    }
   ],
   "source": [
    "# Terminate EC2 Instances:\n",
    "ec2_client.terminate_instances(InstanceIds=[instance.id for instance in instances])\n",
    "\n",
    "# Confirm that EC2 instances were terminated:\n",
    "waiter = ec2_client.get_waiter('instance_terminated')\n",
    "waiter.wait(InstanceIds=[instance.id for instance in instances])\n",
    "print(\"EC2 Instances Successfully Terminated\")\n",
    "\n",
    "# Delete Kinesis Stream (if it currently exists):\n",
    "try:\n",
    "    response = kinesis.delete_stream(StreamName='test_stream')\n",
    "except kinesis.exceptions.ResourceNotFoundException:\n",
    "    pass\n",
    "\n",
    "# Confirm that Kinesis Stream was deleted:\n",
    "waiter = kinesis.get_waiter('stream_not_exists')\n",
    "waiter.wait(StreamName='test_stream')\n",
    "print(\"Kinesis Stream Successfully Deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
